{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"T5FineTurn.ipynb","provenance":[{"file_id":"1UeZOH_TLPhgFAGBtEhgYjD_eKlU-T24I","timestamp":1633429846017},{"file_id":"1nYCo_Hm0fsiPnecJsXxyObAtaGnveA2x","timestamp":1633404966570},{"file_id":"1vnpMoZoenRrWeaxMyfYK4DDbtlBu-M8V","timestamp":1631847158358},{"file_id":"19sc3Q3rQlEtYQcJ6P4zQ-we_HJPav7gv","timestamp":1609343273125}],"collapsed_sections":[]},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"5xk9lb9GmZuS","executionInfo":{"status":"ok","timestamp":1634661896194,"user_tz":-420,"elapsed":7162,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["!pip install transformers  --quiet\n","!pip install sentencepiece==0.1.94 --quiet"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/"},"id":"Iw8DBxi7S28v","executionInfo":{"status":"ok","timestamp":1634661896195,"user_tz":-420,"elapsed":19,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}},"outputId":"868ecded-1ac9-4eb2-cad1-770a0fd5ce24"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"IL6-XP7zzH7h","executionInfo":{"status":"ok","timestamp":1634661899301,"user_tz":-420,"elapsed":3120,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["import torch\n","import pandas as pd\n","import numpy as np\n","from transformers import TrainingArguments, Trainer, AutoTokenizer, T5ForConditionalGeneration, T5Config\n","from transformers.optimization import Adafactor, AdafactorSchedule\n","from torch.utils.data import Dataset"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"pycharm":{"name":"#%%\n"},"id":"Mj6e0bXVbZJV","executionInfo":{"status":"ok","timestamp":1634661899302,"user_tz":-420,"elapsed":16,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["### Config\n","MODEL = {\n","    'name': 't5-small',\n","    'data_link': \"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/POS/learn_data.csv\",\n","    'num_decoder_layers': 6,\n","    'num_freeze': 5\n","}\n","strategy = 'epoch'\n","training_args = TrainingArguments(\n","    output_dir=\"/content/drive/MyDrive\",\n","    overwrite_output_dir=True,\n","    save_strategy=strategy,\n","    disable_tqdm=False,\n","    debug=\"underflow_overflow\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    gradient_accumulation_steps=16,\n","    evaluation_strategy='epoch',\n","    #logging_steps = 16,\n","    #eval_steps=16,\n","    fp16=False,\n","    warmup_steps=100,\n","    learning_rate=1e-3,\n","    adam_epsilon=1e-3,\n","    weight_decay=0.01,\n","    save_total_limit=1,\n","    load_best_model_at_end=False,\n",")\n","\n","def getOptimizer(model):\n","  return Adafactor(model.parameters(), lr=1e-3, relative_step=False, warmup_init=False)\n","\n","def freezeLayer(model, freeze):\n","    for layer in model.base_model.encoder.block[:freeze]:\n","      for param in layer.parameters():\n","          param.requires_grad = False\n","\n","def tokenConfig(tokenizer):\n","    assert tokenizer\n","    tokenizer.padding_side = \"left\"\n","\n","def train_validate_test_split(df, train_percent=.8):\n","    perm = np.random.permutation(df.index)\n","    m = len(df.index)\n","    train_end = int(train_percent * m)\n","    train = df.iloc[perm[:train_end]]\n","    test = df.iloc[perm[train_end:]]\n","    return train, test"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uybva7pDroc","executionInfo":{"status":"ok","timestamp":1634661899819,"user_tz":-420,"elapsed":532,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["data = pd.read_csv(MODEL['data_link'], header=0)\n","data = data.astype(str)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8gp0I8JnMEE","executionInfo":{"status":"ok","timestamp":1634661899820,"user_tz":-420,"elapsed":4,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["class myDataset(Dataset):\n","    def __init__(self, tokenizer, df, max_len=512):\n","        self.data_column = df[\"source\"].values + '</s>'\n","        self.class_column = df['target'].values + '</s>'\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.data_column)\n","\n","    def __getitem__(self, index):\n","        tokenized_inputs = self.tokenizer.encode_plus(self.data_column[index], max_length=self.max_len,\n","                                                      padding='longest', return_tensors=\"pt\")\n","        tokenized_targets = self.tokenizer.encode_plus(self.class_column[index], max_length=4, pad_to_max_length=True,\n","                                                       return_tensors=\"pt\")\n","        source_ids = tokenized_inputs[\"input_ids\"].squeeze()\n","        target_ids = tokenized_targets[\"input_ids\"].squeeze()\n","        src_mask = tokenized_inputs[\"attention_mask\"].squeeze()\n","        return {\"input_ids\": source_ids, \"attention_mask\": src_mask,\n","                \"label\": target_ids}\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeD9TM-OVqbo","executionInfo":{"status":"ok","timestamp":1634661899821,"user_tz":-420,"elapsed":5,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["class StsTrainer(Trainer):\n","  def compute_loss(self,model,inputs,classifier):\n","    output = model.generate(inputs)\n","    text = tok.decode(output)\n","    # convert text to ids\n","    classifier_output = classifier(text)\n","    loss = loss_function(classifier_output, targets)\n","    return loss"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2zrELuFTzEG","executionInfo":{"status":"ok","timestamp":1634661905080,"user_tz":-420,"elapsed":5263,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL['name'])\n","tokenConfig(tokenizer=tokenizer)\n","assert tokenizer\n","\n","config = T5Config.from_pretrained(MODEL['name'])\n","config.num_decoder_layers = MODEL['num_decoder_layers']\n","model = T5ForConditionalGeneration.from_pretrained(MODEL['name'], config=config)\n","freezeLayer(model, MODEL['num_freeze'])\n","\n","#optimizer = getOptimizer(model)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","\n","assert model"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHYTARXZOKwP","colab":{"base_uri":"https://localhost:8080/"},"pycharm":{"name":"#%% split data\n"},"executionInfo":{"status":"ok","timestamp":1634661905080,"user_tz":-420,"elapsed":12,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}},"outputId":"690a683f-7a33-4465-aaf0-629700e73e4f"},"source":["train_data, val_data = train_validate_test_split(data)\n","train_dataset = myDataset(df = train_data, tokenizer = tokenizer)\n","val_dataset = myDataset(df = val_data, tokenizer = tokenizer)\n","\n","assert_data = train_dataset.__getitem__(121)\n","assert_inputs = assert_data['input_ids']\n","assert assert_inputs[-1] == 1\n","assert_label = assert_data['label']\n","assert assert_label[-1] == 1"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hAkcojbJQi8_","executionInfo":{"status":"ok","timestamp":1634661905081,"user_tz":-420,"elapsed":9,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}},"outputId":"350ced7c-f91a-4ae2-d687-a8239224e1c6"},"source":["tokenizer"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PreTrainedTokenizerFast(name_or_path='t5-small', vocab_size=32100, model_max_len=512, is_fast=True, padding_side='left', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']})"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"GsKQJis8jcCh","pycharm":{"name":"#%%\n"},"colab":{"base_uri":"https://localhost:8080/","height":248},"outputId":"662d72ef-f23a-43e8-d104-8d82e8b26568"},"source":["#lr_scheduler = AdafactorSchedule(optimizer)\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,    \n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n",")\n","trainer.train()\n","trainer.save_model()\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 27974\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 16\n","  Total optimization steps = 437\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='51' max='437' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 51/437 06:47 < 53:30, 0.12 it/s, Epoch 0.11/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"ZaQDGHEWsZTO","pycharm":{"name":"#%%\n"},"executionInfo":{"status":"aborted","timestamp":1634661905590,"user_tz":-420,"elapsed":8,"user":{"displayName":"Lộc Kiều","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14708523415990480862"}}},"source":["assert 1 == 0\n","\n"],"execution_count":null,"outputs":[]}]}