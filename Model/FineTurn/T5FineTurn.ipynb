{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of T5 fine tu.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1UeZOH_TLPhgFAGBtEhgYjD_eKlU-T24I",
     "timestamp": 1633429846017
    },
    {
     "file_id": "1nYCo_Hm0fsiPnecJsXxyObAtaGnveA2x",
     "timestamp": 1633404966570
    },
    {
     "file_id": "1vnpMoZoenRrWeaxMyfYK4DDbtlBu-M8V",
     "timestamp": 1631847158358
    },
    {
     "file_id": "19sc3Q3rQlEtYQcJ6P4zQ-we_HJPav7gv",
     "timestamp": 1609343273125
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "5xk9lb9GmZuS",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "!pip install transformers  --quiet\n",
    "!pip install sentencepiece==0.1.94 --quiet"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IL6-XP7zzH7h"
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from transformers import TrainingArguments, Trainer, T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "MODEL = {\n",
    "    'name': 't5-small'\n",
    "}\n",
    "\n",
    "strategy = 'epoch'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_strategy=strategy,\n",
    "    disable_tqdm=False,\n",
    "    debug=\"underflow_overflow\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    evaluation_strategy=strategy,\n",
    "    fp16=False,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-3,\n",
    "    adam_epsilon=1e-8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "\n",
    "def freezeLayer(model):\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def tokenConfig(tokenizer):\n",
    "    tokenizer.padding_side = \"left\"\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_uybva7pDroc"
   },
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/POS/learn_data.csv\", header=0)\n",
    "data = data.astype(str)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I8gp0I8JnMEE"
   },
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, tokenizer, df, max_len=512):\n",
    "        self.data_column = df[\"source\"].values + '</s>'\n",
    "        self.class_column = df['target'].values + '</s>'\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_column)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokenized_inputs = self.tokenizer.encode_plus(self.data_column[index], max_length=self.max_len,\n",
    "                                                      padding='longest', return_tensors=\"pt\")\n",
    "        tokenized_targets = self.tokenizer.encode_plus(self.class_column[index], max_length=4, pad_to_max_length=True,\n",
    "                                                       return_tensors=\"pt\")\n",
    "        source_ids = tokenized_inputs[\"input_ids\"].squeeze()\n",
    "        target_ids = tokenized_targets[\"input_ids\"].squeeze()\n",
    "        src_mask = tokenized_inputs[\"attention_mask\"].squeeze()\n",
    "        return {\"input_ids\": source_ids, \"attention_mask\": src_mask,\n",
    "                \"label\": target_ids}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CwCXZgyrU7P5"
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_validate_test_split(df, train_percent=.8):\n",
    "    perm = np.random.permutation(df.index)\n",
    "    m = len(df.index)\n",
    "    train_end = int(train_percent * m)\n",
    "    train = df.iloc[perm[:train_end]]\n",
    "    test = df.iloc[perm[train_end:]]\n",
    "    return train, test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s2zrELuFTzEG"
   },
   "source": [
    "from transformers import AutoTokenizer, T5ForConditionalGeneration\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenConfig(tokenizer=tokenizer)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.cuda()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HHYTARXZOKwP"
   },
   "source": [
    "train_data, val_data = train_validate_test_split(data)\n",
    "train_dataset = myDataset(df = train_data, tokenizer = tokenizer)\n",
    "val_dataset = myDataset(df = val_data, tokenizer = tokenizer)\n",
    "\n",
    "f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WPMUVfgZMcFG"
   },
   "source": [
    "a = train_dataset.__getitem__(1)\n",
    "b = tokenizer.decode(a['input_ids'])\n",
    "b"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GsKQJis8jcCh"
   },
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer.train()\n",
    "trainer.save_model()    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U211ezK0P5UC"
   },
   "source": [
    "model.eval()\n",
    "model.to('cpu')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_KBLk_3MLa8u"
   },
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "model.to('cpu')\n",
    "\n",
    "test_df = pd.read_csv(\"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/POS/test.csv\", header=0)\n",
    "columns = [\"test_id\", \"expected\", \"actual\"]\n",
    "result_df = pd.DataFrame(columns=columns)\n",
    "task_prefix = 'stsb '\n",
    "tqdm.pandas()\n",
    "for index, row in tqdm(test_df.iterrows(), leave=False):\n",
    "    temp_df = pd.read_csv(\"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/sentence_list.csv\", header=0)\n",
    "    test_sentence = row[\"sentence\"]\n",
    "    for i, r in temp_df.iterrows():\n",
    "        compare_sentences = r[\"sentence\"]\n",
    "        T5_format_sentence = task_prefix + \"sentence1: \" + test_sentence + \". sentence2: \" + compare_sentences\n",
    "        inputs = tokenizer(T5_format_sentence, return_tensors=\"pt\", padding=True)\n",
    "        output_sequences = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'],\n",
    "                                          do_sample=False)\n",
    "        similarity = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "        temp_df.loc[i, \"similarity\"] = similarity\n",
    "    temp_df['similarity'] = pd.to_numeric(temp_df['similarity'], errors='coerce')\n",
    "    mean_df = temp_df.groupby([\"intent_index\"])[\"similarity\"].mean().reset_index()\n",
    "    max_row = mean_df.iloc[mean_df[\"similarity\"].idxmax()]\n",
    "    new_row = {'test_id': row[\"sentence_index\"], 'expected': max_row[\"intent_index\"], 'actual':row[\"intent_index\"]}\n",
    "    result_df = result_df.append(new_row, ignore_index=True)\n",
    "result_df.to_csv(path_or_buf='T5Identity.csv', mode='a')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X_8Au5lpvUFq"
   },
   "source": [
    "result_df"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}