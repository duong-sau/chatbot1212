{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of T5 fine tu.ipynb","private_outputs":true,"provenance":[{"file_id":"1UeZOH_TLPhgFAGBtEhgYjD_eKlU-T24I","timestamp":1633429846017},{"file_id":"1nYCo_Hm0fsiPnecJsXxyObAtaGnveA2x","timestamp":1633404966570},{"file_id":"1vnpMoZoenRrWeaxMyfYK4DDbtlBu-M8V","timestamp":1631847158358},{"file_id":"19sc3Q3rQlEtYQcJ6P4zQ-we_HJPav7gv","timestamp":1609343273125}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ZwRhz144Fknp"},"source":["Related article: https://www.ivanlai.project-ds.net/post/conditional-text-generation-by-fine-tuning-gpt-2\n","\n","Preprocessing code in [this](https://github.com/ivanlai/Conditional_Text_Generation) Github repository."]},{"cell_type":"code","metadata":{"id":"ml2Xz0x348NT"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tSWYe3LMSP-b"},"source":["### Install and import libraries"]},{"cell_type":"code","metadata":{"id":"5xk9lb9GmZuS"},"source":["%%time\n","%%capture\n","!pip install transformers  \n","!pip install sentencepiece==0.1.94"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IL6-XP7zzH7h"},"source":["import os\n","import io\n","import requests\n","import numpy as np\n","import pandas as pd\n","import re\n","import zipfile\n","import random\n","import time\n","import csv\n","import datetime\n","from itertools import compress\n","from collections import Counter, defaultdict\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n","                         AdamW, get_linear_schedule_with_warmup, \\\n","                         TrainingArguments, BeamScorer, Trainer, T5Tokenizer, T5ForConditionalGeneration\n","\n","import torch\n","from torch.utils.data import Dataset, random_split, DataLoader, \\\n","                             RandomSampler, SequentialSampler\n","\n","from IPython.display import clear_output\n","\n","print(f\"PyTorch version: {torch.__version__}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZDy9RClRiQ3"},"source":["### Configurations"]},{"cell_type":"code","metadata":{"id":"QILzrXuoRhaF"},"source":["DEBUG           = False\n","\n","INPUT_DIR       = 'articles'\n","\n","USE_APEX        = True\n","APEX_OPT_LEVEL  = 'O1'\n","\n","MODEL           = 't5-small'     #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n","\n","UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n","                    \n","MAXLEN          = 768  #{768, 1024, 1280, 1600}\n","\n","TRAIN_SIZE      = 0.8\n","\n","if USE_APEX:\n","    TRAIN_BATCHSIZE = 4\n","    BATCH_UPDATE    = 16\n","else:\n","    TRAIN_BATCHSIZE = 2\n","    BATCH_UPDATE    = 32\n","\n","EPOCHS          = 4\n","LR              = 5e-4\n","EPS             = 1e-8\n","WARMUP_STEPS    = 1e2\n","\n","SEED            = 2020"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"740ZIyZXRbWe"},"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(SEED)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_uybva7pDroc"},"source":["data = pd.read_csv(\"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/POS/learn_data.csv\", header=0)\n","data = data.astype(str)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7v4Hb7kmJDED"},"source":["data['target']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UYte7Q6hm58a"},"source":["data.iloc[1]['source']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8gp0I8JnMEE"},"source":["from torch.utils.data import Dataset, DataLoader\n","from transformers import T5Tokenizer\n","\n","class myDataset(Dataset):\n","  def __init__(self, tokenizer,df,  max_len=128):\n","    self.data_column = df[\"source\"].values + '</s>'\n","    self.class_column = df['target'].values + '</s>'\n","    self.max_len = max_len\n","    self.tokenizer = tokenizer\n","        \n","  def __len__(self):\n","      return len(self.data_column)\n","\n","  def __getitem__(self, index):\n","    # tokenize inputs\n","    tokenized_inputs = self.tokenizer.encode_plus( self.data_column[index], max_length=self.max_len, padding='longest', return_tensors=\"pt\")\n","    tokenized_targets = self.tokenizer.encode_plus( self.class_column[index] , max_length=4, pad_to_max_length=True, return_tensors=\"pt\")\n","    source_ids = tokenized_inputs[\"input_ids\"].squeeze()\n","    target_ids = tokenized_targets[\"input_ids\"].squeeze()\n","    src_mask    = tokenized_inputs[\"attention_mask\"].squeeze() # might need to squeeze\n","    target_mask = tokenized_targets['attention_mask'].squeeze()  # might need to squeeze\n","    return {\"input_ids\": source_ids, \"attention_mask\": src_mask, \n","                \"label\": target_ids}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwCXZgyrU7P5"},"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","\n","def train_validate_test_split(df, train_percent=.8, seed=None):\n","    np.random.seed(seed)\n","    perm = np.random.permutation(df.index)\n","    m = len(df.index)\n","    train_end = int(train_percent * m)\n","    train = df.iloc[perm[:train_end]]\n","    test = df.iloc[perm[train_end:]]\n","    return train, test\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v3LfEbc5j9Yo"},"source":["### Loading Tokenizer, Config and Model"]},{"cell_type":"code","metadata":{"id":"s2zrELuFTzEG"},"source":["from transformers import AutoTokenizer, T5ForConditionalGeneration\n","tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n","tokenizer.padding_side = \"left\"\n","type(tokenizer)\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n","model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HHYTARXZOKwP"},"source":["train_data, val_data = train_validate_test_split(data)\n","train_dataset = myDataset(df = train_data, tokenizer = tokenizer)\n","val_dataset = myDataset(df = val_data, tokenizer = tokenizer)\n","\n","f'There are {len(train_dataset) :,} samples for training, and {len(val_dataset) :,} samples for validation testing'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPMUVfgZMcFG"},"source":["a = train_dataset.__getitem__(1)\n","b = tokenizer.decode(a['input_ids'])\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsKQJis8jcCh"},"source":["training_args = TrainingArguments(\n","    output_dir=\"/content/\",\n","    num_train_epochs=5,\n","    per_device_train_batch_size=TRAIN_BATCHSIZE,\n","    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n","    gradient_accumulation_steps=BATCH_UPDATE,\n","    evaluation_strategy=\"epoch\",\n","    fp16=False,\n","    fp16_opt_level=APEX_OPT_LEVEL,\n","    warmup_steps=WARMUP_STEPS,    \n","    learning_rate=LR,\n","    adam_epsilon=EPS,\n","    weight_decay=0.01,        \n","    save_total_limit=1,\n","    load_best_model_at_end=False,     \n",")\n","\n","#---------------------------------------------------#\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,    \n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer\n",")\n","\n","#---------------------------------------------------#\n","trainer.train()\n","trainer.save_model()    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U211ezK0P5UC"},"source":["model.eval()\n","model.to('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImdfcEj8Qbsw"},"source":["da = train_dataset.__getitem__(5)\n","da['input_ids']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4x5NyT4Zlrk"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsF4RgVoLbh6"},"source":["model.cuda()\n","T5_format_sentence = 'stsb '  + \"sentence1: \" + \"this is q sq\" + \". sentence2: \" + \"compare_sentences\"\n","inputs = tokenizer(T5_format_sentence, return_tensors=\"pt\").to('cuda')\n","output_sequences = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","                                          do_sample=False)\n","similarity = tokenizer.batch_decode(output_sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBnoMhGbLng1"},"source":["similarity\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EgEeOPPZJx8"},"source":["output_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_KBLk_3MLa8u"},"source":["from tqdm import tqdm\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import pandas as pd\n","model.to('cpu')\n","\n","test_df = pd.read_csv(\"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/POS/test.csv\", header=0)\n","columns = [\"test_id\", \"expected\", \"actual\"]\n","result_df = pd.DataFrame(columns=columns)\n","task_prefix = 'stsb '\n","tqdm.pandas()\n","for index, row in tqdm(test_df.iterrows(), leave=False):\n","    temp_df = pd.read_csv(\"https://raw.githubusercontent.com/duong-sau/chatbot1212/master/Model/Data/IntentClassification/sentence_list.csv\", header=0)\n","    test_sentence = row[\"sentence\"]\n","    for i, r in temp_df.iterrows():\n","        compare_sentences = r[\"sentence\"]\n","        T5_format_sentence = task_prefix + \"sentence1: \" + test_sentence + \". sentence2: \" + compare_sentences\n","        inputs = tokenizer(T5_format_sentence, return_tensors=\"pt\", padding=True)\n","        output_sequences = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'],\n","                                          do_sample=False)\n","        similarity = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n","        temp_df.loc[i, \"similarity\"] = similarity\n","    temp_df['similarity'] = pd.to_numeric(temp_df['similarity'], errors='coerce')\n","    mean_df = temp_df.groupby([\"intent_index\"])[\"similarity\"].mean().reset_index()\n","    max_row = mean_df.iloc[mean_df[\"similarity\"].idxmax()]\n","    new_row = {'test_id': row[\"sentence_index\"], 'expected': max_row[\"intent_index\"], 'actual':row[\"intent_index\"]}\n","    result_df = result_df.append(new_row, ignore_index=True)\n","result_df.to_csv(path_or_buf='T5Identity.csv', mode='a')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_8Au5lpvUFq"},"source":["result_df"],"execution_count":null,"outputs":[]}]}